{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5021f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 17:57:51.395650: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-19 17:57:51.405500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-19 17:57:51.418570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-19 17:57:51.418602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-19 17:57:51.427230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-19 17:57:51.869218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# XLA-optimized U-Net with ResNet-34 backbone and Parallel-CAM modules\n",
    "# All layers defined once in __init__, static loops unrolled, channel-last format\n",
    "\n",
    "class SpatialSelfAttention(layers.Layer):\n",
    "    def __init__(self, channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.scale = tf.math.sqrt(tf.cast(channels, tf.float32))\n",
    "        self.q_conv = layers.Conv2D(channels, 1)\n",
    "        self.k_conv = layers.Conv2D(channels, 1)\n",
    "        self.v_conv = layers.Conv2D(channels, 1)\n",
    "        self.reshape_to_seq = layers.Reshape((-1, channels))\n",
    "        self.reshape_to_map = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.reshape_to_map = layers.Reshape((input_shape[1], input_shape[2], input_shape[3]))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        q = self.reshape_to_seq(self.q_conv(x))  # [B, H*W, C]\n",
    "        k = self.reshape_to_seq(self.k_conv(x))\n",
    "        v = self.reshape_to_seq(self.v_conv(x))\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / self.scale\n",
    "        attn = tf.nn.softmax(attn)\n",
    "        out = tf.matmul(attn, v)\n",
    "        return self.reshape_to_map(out)\n",
    "    \n",
    "class ChannelSelfAttention(layers.Layer):\n",
    "    def __init__(self, channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.scale = tf.math.sqrt(tf.cast(channels, tf.float32))\n",
    "        self.q_conv = layers.Conv2D(channels, 1)\n",
    "        self.k_conv = layers.Conv2D(channels, 1)\n",
    "        self.v_conv = layers.Conv2D(channels, 1)\n",
    "        self.reshape_to_seq = layers.Reshape((channels, -1))\n",
    "        self.reshape_to_map = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.reshape_to_map = layers.Reshape((input_shape[1], input_shape[2], input_shape[3]))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        q = self.reshape_to_seq(self.q_conv(x))  # [B, C, H*W]\n",
    "        k = self.reshape_to_seq(self.k_conv(x))\n",
    "        v = self.reshape_to_seq(self.v_conv(x))\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / self.scale\n",
    "        attn = tf.nn.softmax(attn)\n",
    "        out = tf.matmul(attn, v)\n",
    "        return self.reshape_to_map(out)\n",
    "\n",
    "\n",
    "class PCAM(layers.Layer):\n",
    "    def __init__(self, channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = self.add_weight('alpha', shape=(), initializer='zeros')\n",
    "        self.beta = self.add_weight('beta', shape=(), initializer='zeros')\n",
    "        self.spatial = SpatialSelfAttention(channels)\n",
    "        self.channel = ChannelSelfAttention(channels)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        sa = self.spatial(x)\n",
    "        ca = self.channel(x)\n",
    "        out = x + self.beta * sa + self.alpha * ca\n",
    "        return out\n",
    "\n",
    "class CAM(layers.Layer):\n",
    "    def __init__(self, channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = self.add_weight('alpha', shape=(), initializer='zeros')\n",
    "        self.channel = ChannelSelfAttention(channels)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        ca = self.channel(x)\n",
    "        return x + self.alpha * ca\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, channels, dropout_rate=0.0, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.conv1 = layers.Conv2D(channels, 3, padding='same', kernel_initializer='he_uniform', use_bias=True)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-6, momentum=0.9)\n",
    "        self.act1 = layers.LeakyReLU(0.2)\n",
    "        self.drop = layers.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.conv2 = layers.Conv2D(channels, 3, padding='same', kernel_initializer='he_uniform', use_bias=True)\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-6, momentum=0.9)\n",
    "        self.act2 = layers.LeakyReLU(0.2)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        y = self.act1(self.bn1(self.conv1(x)))\n",
    "        if self.drop: y = self.drop(y)\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return self.act2(x + y)\n",
    "\n",
    "class ResProjBlock(layers.Layer):\n",
    "    def __init__(self, channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.conv1 = layers.Conv2D(channels, 3, strides=2, padding='same', kernel_initializer='he_uniform', use_bias=True)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-6, momentum=0.9)\n",
    "        self.act1 = layers.LeakyReLU(0.2)\n",
    "        self.conv2 = layers.Conv2D(channels, 3, padding='same', kernel_initializer='he_uniform')\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-6, momentum=0.9)\n",
    "        self.proj = layers.Conv2D(channels, 1, strides=2, padding='same', kernel_initializer='he_uniform', use_bias=True)\n",
    "        self.act2 = layers.LeakyReLU(0.2)\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        y = self.act1(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        r = self.proj(x)\n",
    "        return self.act2(r + y)\n",
    "\n",
    "class Upsample(layers.Layer):\n",
    "    def __init__(self, channels, scale=2, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.conv = layers.Conv2D(channels * (scale**2), 3, padding='same', kernel_initializer='he_uniform', use_bias=True)\n",
    "        self.act = layers.LeakyReLU(0.2)\n",
    "        self.scale = scale\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        x = self.act(self.conv(x))\n",
    "        return tf.nn.depth_to_space(x, self.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122e1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetResnet34Tr(Model):\n",
    "    def __init__(self, input_shape, data_format, C1=64, multipliers=(1,2,4,8)):\n",
    "        super().__init__()\n",
    "        c1 = C1\n",
    "        self.stem = tf.keras.Sequential([\n",
    "            layers.Conv2D(c1, 7, padding='same', kernel_initializer='he_uniform', use_bias=True),\n",
    "            layers.BatchNormalization(epsilon=1e-6, momentum=0.9),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = self._make_encoder_stage(c1 * multipliers[0], 3)\n",
    "        self.enc2 = self._make_encoder_stage(c1 * multipliers[1], 4)\n",
    "        self.enc3 = self._make_encoder_stage(c1 * multipliers[2], 6)\n",
    "        self.enc4 = self._make_encoder_stage(c1 * multipliers[3], 3)\n",
    "        self.bottleneck = self._make_encoder_stage(c1 * multipliers[3], 2)\n",
    "        # # Attention\n",
    "        self.attn_stem = CAM(c1)\n",
    "        self.attn1 = CAM(c1 * multipliers[0])\n",
    "        self.attn2 = PCAM(c1 * multipliers[1])\n",
    "        self.attn3 = PCAM(c1 * multipliers[2])\n",
    "        self.attn4 = PCAM(c1 * multipliers[3])\n",
    "        self.attn_bottleneck = PCAM(c1 * multipliers[3])\n",
    "        # # Decoder\n",
    "        self.up4 = Upsample(c1 * multipliers[3])\n",
    "        self.dec4 = self._make_decoder_stage(c1 * multipliers[3], c1 * multipliers[2])\n",
    "        self.dec3 = self._make_decoder_stage(c1 * multipliers[2], c1 * multipliers[1])\n",
    "        self.dec2 = self._make_decoder_stage(c1 * multipliers[1], c1 * multipliers[0])\n",
    "        self.dec1 = self._make_decoder_stage(c1 * multipliers[0], c1 * multipliers[0])\n",
    "        # # Final conv\n",
    "        self.final_conv = tf.keras.Sequential([\n",
    "            layers.Conv2D(c1, 3, padding='same', kernel_initializer='he_uniform', use_bias=True),\n",
    "            layers.BatchNormalization(epsilon=1e-6, momentum=0.9),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(1, 3, padding='same', kernel_initializer='he_uniform', use_bias=True),\n",
    "        ])\n",
    "\n",
    "    def _make_encoder_stage(self, channels, num_blocks, use_PCAM=True):\n",
    "        layers_list = [ResProjBlock(channels)]\n",
    "        for _ in range(num_blocks):\n",
    "            layers_list.append(ResBlock(channels))\n",
    "        return tf.keras.Sequential(layers_list)\n",
    "\n",
    "    def _make_decoder_stage(self, in_ch, out_ch):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Conv2D(in_ch, 3, padding='same', kernel_initializer='he_uniform', use_bias=True),\n",
    "            layers.BatchNormalization(epsilon=1e-6, momentum=0.9),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            Upsample(out_ch)\n",
    "        ])\n",
    "\n",
    "    #@tf.function(jit_compile=True)\n",
    "    def call(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.stem(x)\n",
    "        x1 = self.enc1(x0)\n",
    "        x2 = self.enc2(x1)\n",
    "        x3 = self.enc3(x2)\n",
    "        x4 = self.enc4(x3)\n",
    "        b  = self.bottleneck(x4)\n",
    "        # Decoder with skip connections\n",
    "        up = self.up4(self.attn_bottleneck(b))\n",
    "        d4 = tf.concat([up, self.attn4(x4)], axis=-1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = tf.concat([d4, self.attn3(x3)], axis=-1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = tf.concat([d3, self.attn2(x2)], axis=-1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = tf.concat([d2, self.attn1(x1)], axis=-1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # #Final conv\n",
    "        out = tf.concat([d1, self.attn_stem(x0)], axis=-1)\n",
    "        out = self.final_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ea81a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 17:57:52.629291: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.658486: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.658523: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.662576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.662614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.662626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.784796: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.784834: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.784840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-04-19 17:57:52.784861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 17:57:52.784878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unet_resnet34_tr\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 256, 256, 64)      12864     \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 128, 128, 64)      301632    \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 64, 64, 128)       1415552   \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 32, 32, 256)       8013568   \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 16, 16, 512)       17846784  \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 8, 8, 512)         14433792  \n",
      "                                                                 \n",
      " cam (CAM)                   multiple                  12481     \n",
      "                                                                 \n",
      " cam_1 (CAM)                 multiple                  12481     \n",
      "                                                                 \n",
      " pcam (PCAM)                 multiple                  99074     \n",
      "                                                                 \n",
      " pcam_1 (PCAM)               multiple                  394754    \n",
      "                                                                 \n",
      " pcam_2 (PCAM)               multiple                  1575938   \n",
      "                                                                 \n",
      " pcam_3 (PCAM)               multiple                  1575938   \n",
      "                                                                 \n",
      " upsample (Upsample)         multiple                  9439232   \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   (None, 32, 32, 256)       9440768   \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (None, 64, 64, 128)       2361088   \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   (None, 128, 128, 64)      590720    \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 256, 256, 64)      221760    \n",
      "                                                                 \n",
      " sequential_10 (Sequential)  (None, 256, 256, 1)       74625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67823051 (258.72 MB)\n",
      "Trainable params: 67795787 (258.62 MB)\n",
      "Non-trainable params: 27264 (106.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = UnetResnet34Tr(input_shape=(256, 256, 4), data_format='channels_last')\n",
    "model.build(input_shape=(None, 256, 256, 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed9460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
