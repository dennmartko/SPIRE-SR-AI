{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter():\n",
    "    def __init__(self, list_dirs_to_datasets, out_dir_path) -> None:\n",
    "        self.listdir_in = list_dirs_to_datasets # Path to directories containing subdirectories of Train/Validation/Test\n",
    "        self.path_to_train = [os.path.join(self.listdir_in[i], \"Train\") for i in range(len(self.listdir_in))]\n",
    "        self.path_to_validation = [os.path.join(self.listdir_in[i], \"Validation\") for i in range(len(self.listdir_in))]\n",
    "        self.path_to_test = [os.path.join(self.listdir_in[i], \"Test\") for i in range(len(self.listdir_in))]\n",
    "        self.class_names = [cname for cname in os.listdir(self.path_to_train[0]) if os.path.isdir(os.path.join(self.path_to_train[0], cname))]\n",
    "        self.train_samples_per_dataset = [len([file for file in os.listdir(os.path.join(train_dir, self.class_names[0])) if file.endswith('.fits')]) for train_dir in self.path_to_train]\n",
    "        self.validation_samples_per_dataset = [len([file for file in os.listdir(os.path.join(val_dir, self.class_names[0])) if file.endswith('.fits')]) for val_dir in self.path_to_validation]\n",
    "        self.test_samples_per_dataset = [len([file for file in os.listdir(os.path.join(test_dir, self.class_names[0])) if file.endswith('.fits')]) for test_dir in self.path_to_test]\n",
    "        \n",
    "        # Check if output directory exists:\n",
    "        self.out_train_path = os.path.join(out_dir_path, \"Train\")\n",
    "        self.out_validation_path = os.path.join(out_dir_path, \"Validation\")\n",
    "        self.out_test_path = os.path.join(out_dir_path, \"Test\")\n",
    "        \n",
    "        ## Train, Validation, Test Main directories\n",
    "        if not os.path.isdir(self.out_train_path):\n",
    "            os.mkdir(self.out_train_path)\n",
    "        if not os.path.isdir(self.out_validation_path):\n",
    "            os.mkdir(self.out_validation_path)\n",
    "        if not os.path.isdir(self.out_test_path):\n",
    "            os.mkdir(self.out_test_path)\n",
    "\n",
    "        ## Class-Directories\n",
    "        for maindir in [self.out_train_path, self.out_validation_path, self.out_test_path]:\n",
    "            for fname in self.class_names:\n",
    "                if not os.path.isdir(os.path.join(maindir, fname)):\n",
    "                    os.mkdir(os.path.join(maindir, fname))\n",
    "\n",
    "        print(\"Directory Tree Created!\")\n",
    "    def Run(self):\n",
    "\n",
    "        glob_train_file_idx = 0\n",
    "        glob_validation_file_idx = 0\n",
    "        glob_test_file_idx = 0\n",
    "\n",
    "        # Merge Training directories\n",
    "        for idx, dir_to_train_dataset in tqdm(enumerate(self.path_to_train), desc=\"Merging Training sub-directories...\"):\n",
    "            for fnum in range(self.train_samples_per_dataset[idx]):\n",
    "                for cnum, fname in enumerate(self.class_names):\n",
    "                    source = os.path.join(dir_to_train_dataset, os.path.join(self.class_names[cnum],f\"{fname}_{fnum}.fits\"))\n",
    "                    dest = os.path.join(os.path.join(self.out_train_path, fname), f\"{fname}_{glob_train_file_idx}.fits\")\n",
    "                    shutil.move(source, dest)\n",
    "\n",
    "                glob_train_file_idx += 1\n",
    "\n",
    "        # Merge validation directories\n",
    "        for idx, dir_to_validation_dataset in tqdm(enumerate(self.path_to_validation), desc=\"Merging Validation sub-directories...\"):\n",
    "            for fnum in range(self.validation_samples_per_dataset[idx]):\n",
    "                for cnum, fname in enumerate(self.class_names):\n",
    "                    source = os.path.join(dir_to_validation_dataset, os.path.join(self.class_names[cnum],f\"{fname}_{fnum}.fits\"))\n",
    "                    dest = os.path.join(os.path.join(self.out_validation_path, fname), f\"{fname}_{glob_validation_file_idx}.fits\")\n",
    "                    shutil.move(source, dest)\n",
    "\n",
    "                glob_validation_file_idx += 1\n",
    "\n",
    "        # Merge Test directories\n",
    "        for idx, dir_to_test_dataset in tqdm(enumerate(self.path_to_test), desc=\"Merging Test sub-directories...\"):\n",
    "            for fnum in range(self.test_samples_per_dataset[idx]):\n",
    "                for cnum, fname in enumerate(self.class_names):\n",
    "                    source = os.path.join(dir_to_test_dataset, os.path.join(self.class_names[cnum],f\"{fname}_{fnum}.fits\"))\n",
    "                    dest = os.path.join(os.path.join(self.out_test_path, fname), f\"{fname}_{glob_test_file_idx}.fits\")\n",
    "                    shutil.move(source, dest)\n",
    "\n",
    "                glob_test_file_idx += 1\n",
    "\n",
    "        print(\"Data Sucessfully Split!\")\n",
    "\n",
    "    def Clean(self):\n",
    "        for old_dir in self.listdir_in:\n",
    "            shutil.rmtree(old_dir)\n",
    "        print(\"Directory Cleaned!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Tree Created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Training sub-directories...: 7it [00:14,  2.01s/it]\n",
      "Merging Validation sub-directories...: 7it [00:00, 27.56it/s]\n",
      "Merging Test sub-directories...: 7it [00:00, 27.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sucessfully Split!\n",
      "Directory Cleaned!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base = r\"/mnt/d/SPIRE-SR-AI/data/processed/50deg_shark_sides_spritz\" #r\"/scratch/p317470/SRHerschel500/data/processed/40deg_sides_spritz_dataset\" #r\"/mnt/d/SRHerschel500/data/processed/NADataset256x256\"#r\"/scratch/p317470/SRHerschel500/data/processed/HNDataset256x256\" #r\"/mnt/d/SRHerschel500/data/processed/tmpDataset256x256\" #r\"/scratch-shared/dkoopmans/Dataset256x256\"\n",
    "SHARK = [f\"SHARK_{i+1}\" for i in range(0, 12)] #[f\"SHARK_{}\" for i in range(4)]\n",
    "SIDES = [f\"SIDES_{i+1}\" for i in range(0, 12)]\n",
    "SPRITZ = [f\"SPRITZ\"]\n",
    "datasets = SHARK + SIDES + SPRITZ # Prefixes of the datamaps. Check the code for \"fname\" for details on standard formatting of files. CTRL + F --> \"fname\"\n",
    "\n",
    "listdir_to_datasets = [os.path.join(base, dataset) for dataset in datasets]\n",
    "\n",
    "splitter = DataSplitter(listdir_to_datasets, base)\n",
    "splitter.Run()\n",
    "splitter.Clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
